#app/core/db/db_upgrade.py
import logging
from sqlalchemy import inspect, text
import hashlib
import json

logger = logging.getLogger(__name__)

class VersionedAutoMigrator:
    """
    å¸¦ç‰ˆæœ¬æ§åˆ¶çš„è‡ªåŠ¨è¿ç§»å™¨ï¼ˆSQLAlchemy Core å…¼å®¹ç‰ˆï¼‰
    """

    def __init__(self, engine, metadata):
        self.engine = engine
        self.metadata = metadata
        self.inspector = inspect(engine)
        self._ensure_migrations_table()

    def _ensure_migrations_table(self):
        """ç¡®ä¿è¿ç§»è®°å½•è¡¨å­˜åœ¨"""
        try:
            if not self.inspector.has_table('_migrations'):
                with self.engine.connect() as conn:
                    conn.execute(text("""
                        CREATE TABLE _migrations (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            table_name VARCHAR(100) NOT NULL,
                            field_name VARCHAR(100) NOT NULL,
                            field_type VARCHAR(50),
                            applied_at TIMESTAMP DEFAULT (datetime('now', 'localtime')),
                            checksum VARCHAR(64),
                            UNIQUE(table_name, field_name)
                        )
                    """))
                    conn.commit()
                logger.info("âœ… åˆ›å»ºè¿ç§»è®°å½•è¡¨")
        except Exception as e:
            logger.error(f"åˆ›å»ºè¿ç§»è®°å½•è¡¨å¤±è´¥: {e}")

    def _get_applied_fields(self, table_name: str):
        """è·å–å·²åº”ç”¨çš„å­—æ®µ"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(
                    text("SELECT field_name, checksum FROM _migrations WHERE table_name = :table"),
                    {"table": table_name}
                )
                return {row[0]: row[1] for row in result}
        except Exception:
            return {}

    def _record_migration(self, table_name: str,
                          field_name: str, field_type: str, checksum: str):
        """è®°å½•è¿ç§»"""
        with self.engine.connect() as conn:
            conn.execute(
                text("""
                    INSERT OR REPLACE INTO _migrations 
                    (table_name, field_name, field_type, checksum)
                    VALUES (:table, :field, :type, :checksum)
                """),
                {
                    "table": table_name,
                    "field": field_name,
                    "type": field_type,
                    "checksum": checksum
                }
            )
            conn.commit()

    def _calculate_checksum(self, column) -> str:
        """è®¡ç®—å­—æ®µçš„æ ¡éªŒå’Œ"""
        col_info = {
            'name': column.name,
            'type': str(column.type),
            'nullable': getattr(column, 'nullable', True),  # Table.Column å¯èƒ½æ²¡æœ‰ nullable
            'default': str(column.default.arg) if hasattr(column, 'default') and column.default else None
        }
        return hashlib.md5(json.dumps(col_info, sort_keys=True).encode()).hexdigest()[:8]

    def sync_table(self, table_name: str, table_obj) -> list:
        """åŒæ­¥å•ä¸ªè¡¨ï¼Œè¿”å›æ–°æ·»åŠ çš„å­—æ®µ"""
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        if not self.inspector.has_table(table_name):
            logger.info(f"ğŸ“¦ åˆ›å»ºè¡¨: {table_name}")
            table_obj.create(self.engine)
            return ['__table_created__']

        # è·å–ç°æœ‰å­—æ®µ
        existing_columns = [
            col['name'] for col in self.inspector.get_columns(table_name)
        ]

        # è·å–å·²è¿ç§»çš„å­—æ®µ
        applied_fields = self._get_applied_fields(table_name)
        added_fields = []

        with self.engine.connect() as conn:
            for column in table_obj.columns:
                # è·³è¿‡ä¸»é”®ï¼ˆå¯é€‰ï¼‰
                # if column.primary_key:
                #     continue

                name = column.name
                # è®¡ç®—å½“å‰å­—æ®µçš„æ ¡éªŒå’Œ
                checksum = self._calculate_checksum(column)

                # å¦‚æœå­—æ®µå·²å­˜åœ¨ä¸”å·²è®°å½•ï¼Œè·³è¿‡
                if name in existing_columns:
                    if name in applied_fields and applied_fields[name] == checksum:
                        continue
                    continue

                # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                try:
                    # SQLite å…¼å®¹çš„å­—æ®µç±»å‹
                    col_type = self._get_sqlite_compatible_type(column)

                    # SQLite çš„ ALTER TABLE ä¸æ”¯æŒ DEFAULTï¼Œæ‰€ä»¥åªæ·»åŠ å­—æ®µ
                    sql = f"ALTER TABLE {table_name} ADD COLUMN {name} {col_type}"

                    conn.execute(text(sql))

                    # è®°å½•è¿ç§»
                    self._record_migration(
                        table_name, name,
                        str(column.type), checksum
                    )

                    logger.info(f"âœ… æ·»åŠ å­—æ®µ: {table_name}.{name} ({col_type})")
                    added_fields.append(name)

                except Exception as e:
                    logger.error(f"âŒ æ·»åŠ å­—æ®µå¤±è´¥ {table_name}.{name}: {e}")

            conn.commit()

        # ä¸ºæ–°å­—æ®µè®¾ç½®é»˜è®¤å€¼
        if added_fields:
            self._update_default_values(conn, table_name, table_obj, added_fields)

        return added_fields

    def _get_sqlite_compatible_type(self, column) -> str:
        """è·å– SQLite å…¼å®¹çš„ç±»å‹"""
        from sqlalchemy import types

        col_type = column.type

        if isinstance(col_type, types.Integer):
            return "INTEGER"
        elif isinstance(col_type, types.String):
            return f"VARCHAR({col_type.length})" if col_type.length else "TEXT"
        elif isinstance(col_type, types.Text):
            return "TEXT"
        elif isinstance(col_type, types.Boolean):
            return "INTEGER"  # SQLite ç”¨ INTEGER è¡¨ç¤º BOOLEAN
        elif isinstance(col_type, types.DateTime):
            return "TIMESTAMP"
        elif isinstance(col_type, types.Date):
            return "DATE"
        elif isinstance(col_type, types.Float):
            return "REAL"
        else:
            return "TEXT"

    def _update_default_values(self, conn, table_name, table_obj, added_fields):
        """ä¸ºæ–°æ·»åŠ çš„å­—æ®µè®¾ç½®é»˜è®¤å€¼"""
        updates = []
        params = {}

        for field_name in added_fields:
            column = table_obj.columns[field_name]
            if hasattr(column, 'default') and column.default is not None:
                default_value = column.default.arg
                if callable(default_value):
                    if 'now' in str(default_value).lower():
                        updates.append(f"{field_name} = CURRENT_TIMESTAMP")
                    else:
                        continue
                else:
                    if isinstance(default_value, bool):
                        updates.append(f"{field_name} = :{field_name}_default")
                        params[f"{field_name}_default"] = 1 if default_value else 0
                    elif isinstance(default_value, (int, float, str)):
                        updates.append(f"{field_name} = :{field_name}_default")
                        params[f"{field_name}_default"] = default_value

        if updates:
            update_sql = f"UPDATE {table_name} SET {', '.join(updates)}"
            try:
                conn.execute(text(update_sql), params)
                conn.commit()
                logger.info(f"âœ… ä¸ºè¡¨ {table_name} çš„æ–°å­—æ®µè®¾ç½®äº†é»˜è®¤å€¼")
            except Exception as e:
                logger.warning(f"âš ï¸ è®¾ç½®é»˜è®¤å€¼å¤±è´¥: {e}")

    def sync_all(self) -> dict:
        """åŒæ­¥æ‰€æœ‰è¡¨"""
        results = {}

        # éå† metadata ä¸­çš„æ‰€æœ‰è¡¨
        for table_name, table_obj in self.metadata.tables.items():
            try:
                added = self.sync_table(table_name, table_obj)
                if added and added != ['__table_created__']:
                    results[table_name] = added
            except Exception as e:
                logger.error(f"åŒæ­¥è¡¨ {table_name} å¤±è´¥: {e}")

        return results
